{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2d8fb4c8-4360-4fdc-b0a2-e1c2e22bd8f9",
      "metadata": {
        "id": "2d8fb4c8-4360-4fdc-b0a2-e1c2e22bd8f9"
      },
      "source": [
        "## Using lifespan models to make predictions on new data\n",
        "\n",
        "This notebook shows how to apply the coefficients from pre-estimated normative models to new data. This can be done in two different ways: (i) using a new set of data derived from the same sites used to estimate the model and (ii) on a completely different set of sites. In the latter case, we also need to estimate the site effect, which requires some calibration/adaptation data. As an illustrative example, we use a dataset derived from several [OpenNeuro datasets](https://openneuro.org/) and adapt the learned model to make predictions on these data.\n",
        "\n",
        "First, if necessary, we install PCNtoolkit (note: this tutorial requires at least version 0.27)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f606eda6",
      "metadata": {
        "collapsed": true,
        "id": "f606eda6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d0f5df-b7bd-4f49-f0b8-b6ae0b8e7e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pcntoolkit in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Requirement already satisfied: bspline<0.2.0,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from pcntoolkit) (0.1.1)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.9.2 in /usr/local/lib/python3.11/dist-packages (from pcntoolkit) (3.10.0)\n",
            "Requirement already satisfied: nibabel<6.0.0,>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from pcntoolkit) (5.3.2)\n",
            "Requirement already satisfied: numba<0.61.0,>=0.60.0 in /usr/local/lib/python3.11/dist-packages (from pcntoolkit) (0.60.0)\n",
            "Requirement already satisfied: nutpie<0.14.0,>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from pcntoolkit) (0.13.4)\n",
            "Requirement already satisfied: pymc<6.0.0,>=5.18.0 in /usr/local/lib/python3.11/dist-packages (from pcntoolkit) (5.21.1)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from pcntoolkit) (1.6.1)\n",
            "Requirement already satisfied: scipy<2.0,>=1.12 in /usr/local/lib/python3.11/dist-packages (from pcntoolkit) (1.14.1)\n",
            "Requirement already satisfied: seaborn<0.14.0,>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from pcntoolkit) (0.13.2)\n",
            "Requirement already satisfied: six<2.0.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from pcntoolkit) (1.17.0)\n",
            "Requirement already satisfied: torch>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from pcntoolkit) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bspline<0.2.0,>=0.1.1->pcntoolkit) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.9.2->pcntoolkit) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.9.2->pcntoolkit) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.9.2->pcntoolkit) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.9.2->pcntoolkit) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.9.2->pcntoolkit) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.9.2->pcntoolkit) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.9.2->pcntoolkit) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.9.2->pcntoolkit) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel<6.0.0,>=5.3.1->pcntoolkit) (6.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel<6.0.0,>=5.3.1->pcntoolkit) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba<0.61.0,>=0.60.0->pcntoolkit) (0.43.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from nutpie<0.14.0,>=0.13.2->pcntoolkit) (18.1.0)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.11/dist-packages (from nutpie<0.14.0,>=0.13.2->pcntoolkit) (2.2.2)\n",
            "Requirement already satisfied: xarray>=2025.1.2 in /usr/local/lib/python3.11/dist-packages (from nutpie<0.14.0,>=0.13.2->pcntoolkit) (2025.1.2)\n",
            "Requirement already satisfied: arviz>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from nutpie<0.14.0,>=0.13.2->pcntoolkit) (0.21.0)\n",
            "Requirement already satisfied: cachetools>=4.2.1 in /usr/local/lib/python3.11/dist-packages (from pymc<6.0.0,>=5.18.0->pcntoolkit) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from pymc<6.0.0,>=5.18.0->pcntoolkit) (3.1.1)\n",
            "Requirement already satisfied: pytensor<2.29,>=2.28.2 in /usr/local/lib/python3.11/dist-packages (from pymc<6.0.0,>=5.18.0->pcntoolkit) (2.28.3)\n",
            "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from pymc<6.0.0,>=5.18.0->pcntoolkit) (13.9.4)\n",
            "Requirement already satisfied: threadpoolctl<4.0.0,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from pymc<6.0.0,>=5.18.0->pcntoolkit) (3.6.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2.0.0,>=1.5.2->pcntoolkit) (1.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.1->pcntoolkit) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.1->pcntoolkit) (1.3.0)\n",
            "Requirement already satisfied: setuptools>=60.0.0 in /usr/local/lib/python3.11/dist-packages (from arviz>=0.20.0->nutpie<0.14.0,>=0.13.2->pcntoolkit) (75.2.0)\n",
            "Requirement already satisfied: h5netcdf>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from arviz>=0.20.0->nutpie<0.14.0,>=0.13.2->pcntoolkit) (1.6.1)\n",
            "Requirement already satisfied: xarray-einstats>=0.3 in /usr/local/lib/python3.11/dist-packages (from arviz>=0.20.0->nutpie<0.14.0,>=0.13.2->pcntoolkit) (0.8.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->nutpie<0.14.0,>=0.13.2->pcntoolkit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->nutpie<0.14.0,>=0.13.2->pcntoolkit) (2025.2)\n",
            "Requirement already satisfied: etuples in /usr/local/lib/python3.11/dist-packages (from pytensor<2.29,>=2.28.2->pymc<6.0.0,>=5.18.0->pcntoolkit) (0.3.9)\n",
            "Requirement already satisfied: logical-unification in /usr/local/lib/python3.11/dist-packages (from pytensor<2.29,>=2.28.2->pymc<6.0.0,>=5.18.0->pcntoolkit) (0.4.6)\n",
            "Requirement already satisfied: miniKanren in /usr/local/lib/python3.11/dist-packages (from pytensor<2.29,>=2.28.2->pymc<6.0.0,>=5.18.0->pcntoolkit) (1.0.3)\n",
            "Requirement already satisfied: cons in /usr/local/lib/python3.11/dist-packages (from pytensor<2.29,>=2.28.2->pymc<6.0.0,>=5.18.0->pcntoolkit) (0.4.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->pymc<6.0.0,>=5.18.0->pcntoolkit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->pymc<6.0.0,>=5.18.0->pcntoolkit) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.1->pcntoolkit) (3.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from h5netcdf>=1.0.2->arviz>=0.20.0->nutpie<0.14.0,>=0.13.2->pcntoolkit) (3.13.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->pymc<6.0.0,>=5.18.0->pcntoolkit) (0.1.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from logical-unification->pytensor<2.29,>=2.28.2->pymc<6.0.0,>=5.18.0->pcntoolkit) (0.12.1)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.11/dist-packages (from logical-unification->pytensor<2.29,>=2.28.2->pymc<6.0.0,>=5.18.0->pcntoolkit) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pcntoolkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ddd7b3cb-b018-4ed4-8b55-15728d8c5411",
      "metadata": {
        "id": "ddd7b3cb-b018-4ed4-8b55-15728d8c5411",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e69b7d-1f38-45ae-9441-89eea7c22341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'braincharts' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/predictive-clinical-neuroscience/braincharts.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b1849f76-b17d-4286-bf57-50ff56e81bf8",
      "metadata": {
        "id": "b1849f76-b17d-4286-bf57-50ff56e81bf8"
      },
      "outputs": [],
      "source": [
        "# we need to be in the scripts folder when we import the libraries in the code block below,\n",
        "# because there is a function called nm_utils that is in the scripts folder that we need to import\n",
        "import os\n",
        "wdir = '/content/braincharts'\n",
        "\n",
        "os.chdir(os.path.join(wdir,'scripts/')) #this path is setup for running on Google Colab. Change it to match your local path if running locally"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2227bc7-e798-470a-99bc-33561ce4511b",
      "metadata": {
        "id": "b2227bc7-e798-470a-99bc-33561ce4511b"
      },
      "source": [
        "Now we import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ff661cf2-7d80-46bb-bcfb-1650a93eed3d",
      "metadata": {
        "id": "ff661cf2-7d80-46bb-bcfb-1650a93eed3d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from pcntoolkit.normative import estimate, predict, evaluate\n",
        "from pcntoolkit.util.utils import compute_MSLL, create_design_matrix\n",
        "from nm_utils import remove_bad_subjects, load_2d"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78719463-28b2-4849-b970-cfbe2f07d214",
      "metadata": {
        "id": "78719463-28b2-4849-b970-cfbe2f07d214"
      },
      "source": [
        "We need to unzip the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3b1d4d4b-68ab-4bba-87f5-6062995805d0",
      "metadata": {
        "id": "3b1d4d4b-68ab-4bba-87f5-6062995805d0"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.join(wdir,'models/'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d4b7b2f4-c514-4d4f-a6b0-9461e1b20831",
      "metadata": {
        "id": "d4b7b2f4-c514-4d4f-a6b0-9461e1b20831",
        "collapsed": true,
        "outputId": "f5167227-7452-41d8-8b1d-eff47a2b54c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  lifespan_DK_46K_59sites.zip\n",
            "replace lifespan_DK_46K_59sites/L_inferiortemporal/Models/meta_data.md? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: lifespan_DK_46K_59sites/L_inferiortemporal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_inferiortemporal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_temporalpole/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_temporalpole/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_postcentral/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_postcentral/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_isthmuscingulate/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_isthmuscingulate/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_superiortemporal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_superiortemporal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_parsopercularis/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_parsopercularis/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_cuneus/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_cuneus/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_caudalanteriorcingulate/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_caudalanteriorcingulate/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_precentral/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_precentral/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_inferiorparietal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_inferiorparietal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_parstriangularis/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_parstriangularis/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_posteriorcingulate/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_posteriorcingulate/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_precentral/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_precentral/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_frontalpole/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_frontalpole/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_temporalpole/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_temporalpole/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_superiorparietal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_superiorparietal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_parahippocampal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_parahippocampal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_caudalanteriorcingulate/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_caudalanteriorcingulate/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_superiortemporal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_superiortemporal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_middletemporal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_middletemporal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_cuneus/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_cuneus/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_isthmuscingulate/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_isthmuscingulate/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_fusiform/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_fusiform/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_pericalcarine/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_pericalcarine/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_rostralanteriorcingulate/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_rostralanteriorcingulate/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_transversetemporal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_transversetemporal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_precuneus/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_precuneus/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_inferiortemporal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_inferiortemporal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_superiorparietal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_superiorparietal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_precuneus/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_precuneus/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_paracentral/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_paracentral/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_fusiform/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_fusiform/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_superiorfrontal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_superiorfrontal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/Median_Thickness/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/Median_Thickness/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_parstriangularis/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_parstriangularis/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_parsorbitalis/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_parsorbitalis/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_inferiorparietal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_inferiorparietal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_supramarginal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_supramarginal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_lingual/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_lingual/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_posteriorcingulate/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_posteriorcingulate/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_rostralmiddlefrontal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_rostralmiddlefrontal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_caudalmiddlefrontal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_caudalmiddlefrontal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_frontalpole/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_frontalpole/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_parsopercularis/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_parsopercularis/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_bankssts/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_bankssts/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_medialorbitofrontal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_medialorbitofrontal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_bankssts/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_bankssts/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_rostralmiddlefrontal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_rostralmiddlefrontal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_rostralanteriorcingulate/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_rostralanteriorcingulate/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/Mean_Thickness/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/Mean_Thickness/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_lateraloccipital/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_lateraloccipital/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_parahippocampal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_parahippocampal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_postcentral/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_postcentral/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_medialorbitofrontal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_medialorbitofrontal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_middletemporal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_middletemporal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_pericalcarine/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_pericalcarine/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_lateralorbitofrontal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_lateralorbitofrontal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_paracentral/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_paracentral/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_entorhinal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_entorhinal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_lateralorbitofrontal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_lateralorbitofrontal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_lateraloccipital/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_lateraloccipital/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_insula/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_insula/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_superiorfrontal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_superiorfrontal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_caudalmiddlefrontal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_caudalmiddlefrontal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_entorhinal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_entorhinal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_supramarginal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_supramarginal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/R_lingual/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/R_lingual/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_parsorbitalis/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_parsorbitalis/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_transversetemporal/Models/NM_0_0_estimate.pkl  \n",
            "  inflating: lifespan_DK_46K_59sites/L_transversetemporal/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_insula/Models/meta_data.md  \n",
            "  inflating: lifespan_DK_46K_59sites/L_insula/Models/NM_0_0_estimate.pkl  \n"
          ]
        }
      ],
      "source": [
        "# we will use the biggest sample as our training set (approx. N=57000 subjects from 82 sites)\n",
        "# for more info on the other pretrained models available in this repository,\n",
        "# please refer to the accompanying paper https://elifesciences.org/articles/72904\n",
        "! unzip lifespan_DK_46K_59sites.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "802b1da6-04cc-4310-af81-f50d38c3e653",
      "metadata": {
        "id": "802b1da6-04cc-4310-af81-f50d38c3e653"
      },
      "source": [
        "Next, we configure some basic variables, like where we want the analysis to be done and which model we want to use.\n",
        "\n",
        "**Note:** We maintain a list of site ids for each dataset, which describe the site names in the training and test data (`site_ids_tr` and `site_ids_te`), plus also the adaptation data . The training site ids are provided as a text file in the distribution and the test ids are extracted automatically from the pandas dataframe (see below). If you use additional data from the sites (e.g. later waves from ABCD), it may be necessary to adjust the site names to match the names in the training set. See the accompanying paper for more details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f52e2a19-9b63-4f0f-97c1-387f1a1872a2",
      "metadata": {
        "id": "f52e2a19-9b63-4f0f-97c1-387f1a1872a2"
      },
      "outputs": [],
      "source": [
        "# which model do we wish to use?\n",
        "model_name = 'lifespan_DK_46K_59sites'\n",
        "site_names = 'site_ids_ct_dk_59sites.txt'\n",
        "\n",
        "# where the analysis takes place\n",
        "root_dir = wdir\n",
        "\n",
        "# where the data files live\n",
        "data_dir = os.path.join(wdir,'docs')\n",
        "\n",
        "# where the models live\n",
        "out_dir = os.path.join(root_dir, 'models', model_name)\n",
        "\n",
        "# load a set of site ids from this model. This must match the training data\n",
        "with open(os.path.join(root_dir,'docs', site_names)) as f:\n",
        "    site_ids_tr = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aab54a5-2579-48d8-a81b-bbd34cea1213",
      "metadata": {
        "id": "3aab54a5-2579-48d8-a81b-bbd34cea1213"
      },
      "source": [
        "### Load test data\n",
        "\n",
        "**Note:** For the purposes of this tutorial, we make predictions for a multi-site transfer dataset, derived from [OpenNeuro](https://openneuro.org/)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: upload files from my device to docs folders\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  !mv -f $fn /content/braincharts/docs\n"
      ],
      "metadata": {
        "id": "PrNR_DlQ-6T4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "55f39b6e-1a2f-4a92-d270-18a7f39f26d4"
      },
      "id": "PrNR_DlQ-6T4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed existing file: site_ids_ct_82sites.txt\n",
            "Removed existing file: OpenNeuroTransfer_sa_te.csv\n",
            "Removed existing file: phenotypes_sc.txt\n",
            "Removed existing file: phenotypes_smith10.txt\n",
            "Removed existing file: phenotypes_sa_rh.txt\n",
            "Removed existing file: phenotypes_ct_dk_rh.txt\n",
            "Removed existing file: site_ids_ct_59sites.txt\n",
            "Removed existing file: phenotypes_ct_dk_mean.txt\n",
            "Removed existing file: phenotypes_sa_lh.txt\n",
            "Removed existing file: OpenNeuroTransfer_yeo17_te.csv\n",
            "Removed existing file: phenotypes_ct_lh.txt\n",
            "Removed existing file: OpenNeuroTransfer_sa_ad.csv\n",
            "Removed existing file: phenotypes_yeo17.txt\n",
            "Removed existing file: OpenNeuroTransfer_ct_te.csv\n",
            "Removed existing file: OpenNeuroTransfer_ct_ad.csv\n",
            "Removed existing file: elife_press_release_photo.jpg\n",
            "Removed existing file: site_ids_yeo17_40sites.txt\n",
            "Removed existing file: site_ids_ct_dk_59sites.txt\n",
            "Removed existing file: site_ids_ct_57sites.txt\n",
            "Removed existing file: all_age_eLife.csv\n",
            "Removed existing file: phenotypes_sa_dk_rh.txt\n",
            "Removed existing file: phenotypes_fa.txt\n",
            "Removed existing file: phenotypes_sa_dk_lh.txt\n",
            "Removed existing file: eLife_interactive_viz.gif\n",
            "Removed existing file: phenotypes_ct_dk_lh.txt\n",
            "Removed existing file: site_ids_sa_66sites.txt\n",
            "Removed existing file: site_ids_fa_19sites.txt\n",
            "Removed existing file: phenotypes_ct_rh.txt\n",
            "Removed existing file: OpenNeuroTransfer_yeo17_ad.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4f7db4e8-d28a-433e-bf39-a74ee6e70e9b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4f7db4e8-d28a-433e-bf39-a74ee6e70e9b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "262d429a-160b-4ba3-9ba4-9acc195bc644",
      "metadata": {
        "id": "262d429a-160b-4ba3-9ba4-9acc195bc644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "dbc72c4f-cb53-4bc6-bcdb-e1459503570a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/braincharts/docs/TLE_inputdata_all.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-8f162d30291c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TLE_inputdata_all.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# extract a list of unique site ids from the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/braincharts/docs/TLE_inputdata_all.csv'"
          ]
        }
      ],
      "source": [
        "test_data = os.path.join(data_dir, 'TLE_inputdata_all.csv')\n",
        "\n",
        "df_te = pd.read_csv(test_data)\n",
        "\n",
        "# extract a list of unique site ids from the test set\n",
        "site_ids_te = sorted(set(df_te['site'].astype(str).to_list()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c636509a-8b12-43f1-811c-08cb22640be2",
      "metadata": {
        "id": "c636509a-8b12-43f1-811c-08cb22640be2"
      },
      "source": [
        "### (Optional) Load adaptation data\n",
        "\n",
        "If the data you wish to make predictions for is not derived from the same scanning sites as those in the trainig set, it is necessary to learn the site effect so that we can account for it in the predictions. In order to do this in an unbiased way, we use a separate dataset, which we refer to as 'adaptation' data. This must contain data for all the same sites as in the test dataset and we assume these are coded in the same way, based on a the 'sitenum' column in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53551023-aff6-4934-ad2d-d77bc63c562d",
      "metadata": {
        "id": "53551023-aff6-4934-ad2d-d77bc63c562d"
      },
      "outputs": [],
      "source": [
        "adaptation_data = os.path.join(data_dir, 'TLE_inputdata_adaption.csv')\n",
        "\n",
        "df_ad = pd.read_csv(adaptation_data)\n",
        "\n",
        "# extract a list of unique site ids from the test set\n",
        "site_ids_ad =  sorted(set(df_ad['site'].astype(str).to_list()))\n",
        "\n",
        "if not all(elem in site_ids_ad for elem in site_ids_te):\n",
        "    print('Warning: some of the testing sites are not in the adaptation data')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ad = df_ad.rename(columns={'site_id': 'sitenum'})\n",
        "df_te = df_te.rename(columns={'site_id': 'sitenum'})"
      ],
      "metadata": {
        "id": "Ny19qPLe4WRa"
      },
      "id": "Ny19qPLe4WRa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4f73e30e-c693-44b8-98c6-52b71b577ea8",
      "metadata": {
        "id": "4f73e30e-c693-44b8-98c6-52b71b577ea8"
      },
      "source": [
        "### Configure which models to fit\n",
        "\n",
        "Now, we configure which imaging derived phenotypes (IDPs) we would like to process. This is just a list of column names in the dataframe we have loaded above.\n",
        "\n",
        "We could load the whole set (i.e. all phenotypes for which we have models for ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b48e104c-cbac-4ae2-8377-cd3ff80162fd",
      "metadata": {
        "id": "b48e104c-cbac-4ae2-8377-cd3ff80162fd"
      },
      "outputs": [],
      "source": [
        "# load the list of idps for left and right hemispheres, plus subcortical regions\n",
        "with open(os.path.join(data_dir, 'phenotypes_ct_dk_lh.txt')) as f:\n",
        "    idp_ids_lh = f.read().splitlines()\n",
        "with open(os.path.join(data_dir, 'phenotypes_ct_dk_rh.txt')) as f:\n",
        "    idp_ids_rh = f.read().splitlines()\n",
        "with open(os.path.join(data_dir, 'phenotypes_ct_dk_mean.txt')) as f:\n",
        "    idp_ids_sc = f.read().splitlines()\n",
        "\n",
        "# we choose here to process all idps\n",
        "idp_ids = idp_ids_lh + idp_ids_rh #+ idp_ids_sc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "280731ad-47d8-43e2-8cb5-4eccfd9f3f81",
      "metadata": {
        "id": "280731ad-47d8-43e2-8cb5-4eccfd9f3f81"
      },
      "source": [
        "... or alternatively, we could just specify a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b74d75f-77a5-474a-9c9b-29aab1ce53a2",
      "metadata": {
        "id": "8b74d75f-77a5-474a-9c9b-29aab1ce53a2"
      },
      "outputs": [],
      "source": [
        "#idp_ids = [ 'Left-Thalamus-Proper', 'Left-Lateral-Ventricle', 'rh_MeanThickness_thickness']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56ee1f7f-8684-4f1c-b142-a68176407029",
      "metadata": {
        "id": "56ee1f7f-8684-4f1c-b142-a68176407029"
      },
      "source": [
        "### Configure covariates\n",
        "\n",
        "Now, we configure some parameters to fit the model. First, we choose which columns of the pandas dataframe contain the covariates (age and sex). The site parameters are configured automatically later on by the `configure_design_matrix()` function, when we loop through the IDPs in the list\n",
        "\n",
        "The supplied coefficients are derived from a 'warped' Bayesian linear regression model, which uses a nonlinear warping function to model non-Gaussianity (`sinarcsinh`) plus a non-linear basis expansion (a cubic b-spline basis set with 5 knot points, which is the default value in the PCNtoolkit package). Since we are sticking with the default value, we do not need to specify any parameters for this, but we do need to specify the limits. We choose to pad the input by a few years either side of the input range. We will also set a couple of options that control the estimation of the model\n",
        "\n",
        "For further details about the likelihood warping approach, see the accompanying paper and [Fraza et al 2021](https://www.biorxiv.org/content/10.1101/2021.04.05.438429v1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62312b8e-4972-4238-abf9-87d9bb33cc10",
      "metadata": {
        "id": "62312b8e-4972-4238-abf9-87d9bb33cc10"
      },
      "outputs": [],
      "source": [
        "# which data columns do we wish to use as covariates?\n",
        "cols_cov = ['age','sex']\n",
        "\n",
        "# limits for cubic B-spline basis\n",
        "xmin = -5\n",
        "xmax = 110\n",
        "\n",
        "# Absolute Z treshold above which a sample is considered to be an outlier (without fitting any model)\n",
        "outlier_thresh = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42bc1072-e9ed-4f2a-9fdd-cbd626a61542",
      "metadata": {
        "id": "42bc1072-e9ed-4f2a-9fdd-cbd626a61542"
      },
      "source": [
        "### Make predictions\n",
        "\n",
        "This will make predictions for each IDP separately. This is done by extracting a column from the dataframe (i.e. specifying the IDP as the response variable) and saving it as a numpy array. Then, we configure the covariates, which is a numpy data array having the number of rows equal to the number of datapoints in the test set. The columns are specified as follows:\n",
        "\n",
        "- A global intercept (column of ones)\n",
        "- The covariate columns (here age and sex, coded as 0=female/1=male)\n",
        "- Dummy coded columns for the sites in the training set (one column per site)\n",
        "- Columns for the basis expansion (seven columns for the default parameterisation)\n",
        "\n",
        "Once these are saved as numpy arrays in ascii format (as here) or (alternatively) in pickle format, these are passed as inputs to the `predict()` method in the PCNtoolkit normative modelling framework. These are written in the same format to the location specified by `idp_dir`. At the end of this step, we have a set of predictions and Z-statistics for the test dataset that we can take forward to further analysis.\n",
        "\n",
        "Note that when we need to make predictions on new data, the procedure is more involved, since we need to prepare, process and store covariates, response variables and site ids for the adaptation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b7471b-c334-464f-8273-b409b7acaac2",
      "metadata": {
        "id": "07b7471b-c334-464f-8273-b409b7acaac2"
      },
      "outputs": [],
      "source": [
        "for idp_num, idp in enumerate(idp_ids):\n",
        "    print('Running IDP', idp_num, idp, ':')\n",
        "    idp_dir = os.path.join(out_dir, idp)\n",
        "    os.chdir(idp_dir)\n",
        "\n",
        "    # extract and save the response variables for the test set\n",
        "    y_te = df_te[idp].to_numpy()\n",
        "\n",
        "    # save the variables\n",
        "    resp_file_te = os.path.join(idp_dir, 'resp_te.txt')\n",
        "    np.savetxt(resp_file_te, y_te)\n",
        "\n",
        "    # configure and save the design matrix\n",
        "    cov_file_te = os.path.join(idp_dir, 'cov_bspline_te.txt')\n",
        "    X_te = create_design_matrix(df_te[cols_cov],\n",
        "                                site_ids = df_te['site'],\n",
        "                                all_sites = site_ids_tr,\n",
        "                                basis = 'bspline',\n",
        "                                xmin = xmin,\n",
        "                                xmax = xmax)\n",
        "    np.savetxt(cov_file_te, X_te)\n",
        "\n",
        "    # check whether all sites in the test set are represented in the training set\n",
        "    if all(elem in site_ids_tr for elem in site_ids_te):\n",
        "        print('All sites are present in the training data')\n",
        "\n",
        "        # just make predictions\n",
        "        yhat_te, s2_te, Z = predict(cov_file_te,\n",
        "                                    alg='blr',\n",
        "                                    respfile=resp_file_te,\n",
        "                                    model_path=os.path.join(idp_dir,'Models'))\n",
        "    else:\n",
        "        print('Some sites missing from the training data. Adapting model')\n",
        "\n",
        "        # save the covariates for the adaptation data\n",
        "        X_ad = create_design_matrix(df_ad[cols_cov],\n",
        "                                    site_ids = df_ad['site'],\n",
        "                                    all_sites = site_ids_tr,\n",
        "                                    basis = 'bspline',\n",
        "                                    xmin = xmin,\n",
        "                                    xmax = xmax)\n",
        "        cov_file_ad = os.path.join(idp_dir, 'cov_bspline_ad.txt')\n",
        "        np.savetxt(cov_file_ad, X_ad)\n",
        "\n",
        "        # save the responses for the adaptation data\n",
        "        resp_file_ad = os.path.join(idp_dir, 'resp_ad.txt')\n",
        "        y_ad = df_ad[idp].to_numpy()\n",
        "        np.savetxt(resp_file_ad, y_ad)\n",
        "\n",
        "        # save the site ids for the adaptation data\n",
        "        sitenum_file_ad = os.path.join(idp_dir, 'sitenum_ad.txt')\n",
        "        site_num_ad = df_ad['sitenum'].to_numpy(dtype=int)\n",
        "        np.savetxt(sitenum_file_ad, site_num_ad)\n",
        "\n",
        "        # save the site ids for the test data\n",
        "        sitenum_file_te = os.path.join(idp_dir, 'sitenum_te.txt')\n",
        "        site_num_te = df_te['sitenum'].to_numpy(dtype=int)\n",
        "        np.savetxt(sitenum_file_te, site_num_te)\n",
        "\n",
        "        yhat_te, s2_te, Z = predict(cov_file_te,\n",
        "                                    alg = 'blr',\n",
        "                                    respfile = resp_file_te,\n",
        "                                    model_path = os.path.join(idp_dir,'Models'),\n",
        "                                    adaptrespfile = resp_file_ad,\n",
        "                                    adaptcovfile = cov_file_ad,\n",
        "                                    adaptvargroupfile = sitenum_file_ad,\n",
        "                                    testvargroupfile = sitenum_file_te)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: shape of adaption and test data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the adaptation data\n",
        "adaptation_data = os.path.join(data_dir, 'TLE_inputdata_adaption.csv')\n",
        "df_ad = pd.read_csv(adaptation_data)\n",
        "\n",
        "# Load the test data\n",
        "test_data = os.path.join(data_dir, 'TLE_inputdata_all.csv')\n",
        "df_te = pd.read_csv(test_data)\n",
        "\n",
        "# Print the shape of the adaptation data\n",
        "print(\"Adaptation data shape:\", df_ad.shape)\n",
        "\n",
        "# Print the shape of the test data\n",
        "print(\"Test data shape:\", df_te.shape)\n"
      ],
      "metadata": {
        "id": "aJe2nTKaE3m8"
      },
      "id": "aJe2nTKaE3m8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "75210821-ccb8-4bd2-82f3-641708811b21",
      "metadata": {
        "id": "75210821-ccb8-4bd2-82f3-641708811b21"
      },
      "source": [
        "### Preparing dummy data for plotting\n",
        "\n",
        "Now, we plot the centiles of variation estimated by the normative model.\n",
        "\n",
        "We do this by making use of a set of dummy covariates that span the whole range of the input space (for age) for a fixed value of the other covariates (e.g. sex) so that we can make predictions for these dummy data points, then plot them. We configure these dummy predictions using the same procedure as we used for the real data. We can use the same dummy data for all the IDPs we wish to plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d0743d8-28ca-4a14-8ef0-99bf40434b5b",
      "metadata": {
        "id": "2d0743d8-28ca-4a14-8ef0-99bf40434b5b"
      },
      "outputs": [],
      "source": [
        "# which sex do we want to plot?\n",
        "sex = 1 # 1 = male 0 = female\n",
        "if sex == 1:\n",
        "    clr = 'blue';\n",
        "else:\n",
        "    clr = 'red'\n",
        "\n",
        "# create dummy data for visualisation\n",
        "print('configuring dummy data ...')\n",
        "xx = np.arange(xmin, xmax, 0.5)\n",
        "X0_dummy = np.zeros((len(xx), 2))\n",
        "X0_dummy[:,0] = xx\n",
        "X0_dummy[:,1] = sex\n",
        "\n",
        "# create the design matrix\n",
        "X_dummy = create_design_matrix(X0_dummy, xmin=xmin, xmax=xmax, site_ids=None, all_sites=site_ids_tr)\n",
        "\n",
        "# save the dummy covariates\n",
        "cov_file_dummy = os.path.join(out_dir,'cov_bspline_dummy_mean.txt')\n",
        "np.savetxt(cov_file_dummy, X_dummy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "126323a3-2270-4796-97c4-94629730ddf7",
      "metadata": {
        "id": "126323a3-2270-4796-97c4-94629730ddf7"
      },
      "source": [
        "### Plotting the normative models\n",
        "\n",
        "Now we loop through the IDPs, plotting each one separately. The outputs of this step are a set of quantitative regression metrics for each IDP and a set of centile curves which we plot the test data against.\n",
        "\n",
        "This part of the code is relatively complex because we need to keep track of many quantities for the plotting. We also need to remember whether the data need to be warped or not. By default in PCNtoolkit, predictions in the form of `yhat, s2` are always in the warped (Gaussian) space. If we want predictions in the input (non-Gaussian) space, then we need to warp them with the inverse of the estimated warping function. This can be done using the function `nm.blr.warp.warp_predictions()`.\n",
        "\n",
        "**Note:** it is necessary to update the intercept for each of the sites. For purposes of visualisation, here we do this by adjusting the median of the data to match the dummy predictions, but note that all the quantitative metrics are estimated using the predictions that are adjusted properly using a learned offset (or adjusted using a hold-out adaptation set, as above). Note also that for the calibration data we require at least two data points of the same sex in each site to be able to estimate the variance. Of course, in a real example, you would want many more than just two since we need to get a reliable estimate of the variance for each site."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdd68cc6-212b-4149-b86a-24e842078e1a",
      "metadata": {
        "id": "cdd68cc6-212b-4149-b86a-24e842078e1a"
      },
      "outputs": [],
      "source": [
        "sns.set(style='whitegrid')\n",
        "\n",
        "for idp_num, idp in enumerate(idp_ids):\n",
        "    print('Running IDP', idp_num, idp, ':')\n",
        "    idp_dir = os.path.join(out_dir, idp)\n",
        "    os.chdir(idp_dir)\n",
        "\n",
        "    # load the true data points\n",
        "    yhat_te = load_2d(os.path.join(idp_dir, 'yhat_predict.txt'))\n",
        "    s2_te = load_2d(os.path.join(idp_dir, 'ys2_predict.txt'))\n",
        "    y_te = load_2d(os.path.join(idp_dir, 'resp_te.txt'))\n",
        "\n",
        "    # set up the covariates for the dummy data\n",
        "    print('Making predictions with dummy covariates (for visualisation)')\n",
        "    yhat, s2 = predict(cov_file_dummy,\n",
        "                       alg = 'blr',\n",
        "                       respfile = None,\n",
        "                       model_path = os.path.join(idp_dir,'Models'),\n",
        "                       outputsuffix = '_dummy')\n",
        "\n",
        "    # load the normative model\n",
        "    with open(os.path.join(idp_dir,'Models', 'NM_0_0_estimate.pkl'), 'rb') as handle:\n",
        "        nm = pickle.load(handle)\n",
        "\n",
        "    # get the warp and warp parameters\n",
        "    W = nm.blr.warp\n",
        "    warp_param = nm.blr.hyp[1:nm.blr.warp.get_n_params()+1]\n",
        "\n",
        "    # first, we warp predictions for the true data and compute evaluation metrics\n",
        "    med_te = W.warp_predictions(np.squeeze(yhat_te), np.squeeze(s2_te), warp_param)[0]\n",
        "    med_te = med_te[:, np.newaxis]\n",
        "    print('metrics:', evaluate(y_te, med_te))\n",
        "\n",
        "    # then, we warp dummy predictions to create the plots\n",
        "    med, pr_int = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param)\n",
        "\n",
        "    # extract the different variance components to visualise\n",
        "    beta, junk1, junk2 = nm.blr._parse_hyps(nm.blr.hyp, X_dummy)\n",
        "    s2n = 1/beta # variation (aleatoric uncertainty)\n",
        "    s2s = s2-s2n # modelling uncertainty (epistemic uncertainty)\n",
        "\n",
        "    # plot the data points\n",
        "    y_te_rescaled_all = np.zeros_like(y_te)\n",
        "    for sid, site in enumerate(site_ids_te):\n",
        "        # plot the true test data points\n",
        "        if all(elem in site_ids_tr for elem in site_ids_te):\n",
        "            # all data in the test set are present in the training set\n",
        "\n",
        "            # first, we select the data points belonging to this particular site\n",
        "            idx = np.where(np.bitwise_and(X_te[:,2] == sex, X_te[:,sid+len(cols_cov)+1] !=0))[0]\n",
        "            if len(idx) == 0:\n",
        "                print('No data for site', sid, site, 'skipping...')\n",
        "                continue\n",
        "\n",
        "            # then directly adjust the data\n",
        "            idx_dummy = np.bitwise_and(X_dummy[:,1] > X_te[idx,1].min(), X_dummy[:,1] < X_te[idx,1].max())\n",
        "            y_te_rescaled = y_te[idx] - np.median(y_te[idx]) + np.median(med[idx_dummy])\n",
        "        else:\n",
        "            # we need to adjust the data based on the adaptation dataset\n",
        "\n",
        "            # first, select the data point belonging to this particular site\n",
        "            idx = np.where(np.bitwise_and(X_te[:,2] == sex, (df_te['site'] == site).to_numpy()))[0]\n",
        "\n",
        "            # load the adaptation data\n",
        "            y_ad = load_2d(os.path.join(idp_dir, 'resp_ad.txt'))\n",
        "            X_ad = load_2d(os.path.join(idp_dir, 'cov_bspline_ad.txt'))\n",
        "            idx_a = np.where(np.bitwise_and(X_ad[:,2] == sex, (df_ad['site'] == site).to_numpy()))[0]\n",
        "            if len(idx) < 2 or len(idx_a) < 2:\n",
        "                print('Insufficent data for site', sid, site, 'skipping...')\n",
        "                continue\n",
        "\n",
        "            # adjust and rescale the data\n",
        "            y_te_rescaled, s2_rescaled = nm.blr.predict_and_adjust(nm.blr.hyp,\n",
        "                                                                   X_ad[idx_a,:],\n",
        "                                                                   np.squeeze(y_ad[idx_a]),\n",
        "                                                                   Xs=None,\n",
        "                                                                   ys=np.squeeze(y_te[idx]))\n",
        "        # plot the (adjusted) data points\n",
        "        plt.scatter(X_te[idx,1], y_te_rescaled, s=4, color=clr, alpha = 0.1)\n",
        "\n",
        "    # plot the median of the dummy data\n",
        "    plt.plot(xx, med, clr)\n",
        "\n",
        "    # fill the gaps in between the centiles\n",
        "    junk, pr_int25 = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param, percentiles=[0.25,0.75])\n",
        "    junk, pr_int95 = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param, percentiles=[0.05,0.95])\n",
        "    junk, pr_int99 = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param, percentiles=[0.01,0.99])\n",
        "    plt.fill_between(xx, pr_int25[:,0], pr_int25[:,1], alpha = 0.1,color=clr)\n",
        "    plt.fill_between(xx, pr_int95[:,0], pr_int95[:,1], alpha = 0.1,color=clr)\n",
        "    plt.fill_between(xx, pr_int99[:,0], pr_int99[:,1], alpha = 0.1,color=clr)\n",
        "\n",
        "    # make the width of each centile proportional to the epistemic uncertainty\n",
        "    junk, pr_int25l = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2-0.5*s2s), warp_param, percentiles=[0.25,0.75])\n",
        "    junk, pr_int95l = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2-0.5*s2s), warp_param, percentiles=[0.05,0.95])\n",
        "    junk, pr_int99l = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2-0.5*s2s), warp_param, percentiles=[0.01,0.99])\n",
        "    junk, pr_int25u = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2+0.5*s2s), warp_param, percentiles=[0.25,0.75])\n",
        "    junk, pr_int95u = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2+0.5*s2s), warp_param, percentiles=[0.05,0.95])\n",
        "    junk, pr_int99u = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2+0.5*s2s), warp_param, percentiles=[0.01,0.99])\n",
        "    plt.fill_between(xx, pr_int25l[:,0], pr_int25u[:,0], alpha = 0.3,color=clr)\n",
        "    plt.fill_between(xx, pr_int95l[:,0], pr_int95u[:,0], alpha = 0.3,color=clr)\n",
        "    plt.fill_between(xx, pr_int99l[:,0], pr_int99u[:,0], alpha = 0.3,color=clr)\n",
        "    plt.fill_between(xx, pr_int25l[:,1], pr_int25u[:,1], alpha = 0.3,color=clr)\n",
        "    plt.fill_between(xx, pr_int95l[:,1], pr_int95u[:,1], alpha = 0.3,color=clr)\n",
        "    plt.fill_between(xx, pr_int99l[:,1], pr_int99u[:,1], alpha = 0.3,color=clr)\n",
        "\n",
        "    # plot actual centile lines\n",
        "    plt.plot(xx, pr_int25[:,0],color=clr, linewidth=0.5)\n",
        "    plt.plot(xx, pr_int25[:,1],color=clr, linewidth=0.5)\n",
        "    plt.plot(xx, pr_int95[:,0],color=clr, linewidth=0.5)\n",
        "    plt.plot(xx, pr_int95[:,1],color=clr, linewidth=0.5)\n",
        "    plt.plot(xx, pr_int99[:,0],color=clr, linewidth=0.5)\n",
        "    plt.plot(xx, pr_int99[:,1],color=clr, linewidth=0.5)\n",
        "\n",
        "    plt.xlabel('Age')\n",
        "    plt.ylabel(idp)\n",
        "    plt.title(idp)\n",
        "    plt.xlim((0,90))\n",
        "    plt.savefig(os.path.join(idp_dir, 'centiles_' + str(sex)),  bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "os.chdir(out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "135dbebd-f563-4a2a-9f44-f96757fb4b0b",
      "metadata": {
        "id": "135dbebd-f563-4a2a-9f44-f96757fb4b0b"
      },
      "outputs": [],
      "source": [
        "# explore an example output folder of a single model (one ROI)\n",
        "# think about what each of these output files represents.\n",
        "# Hint: look at the variable names and comments in the code block above\n",
        "! ls rh_MeanThickness_thickness/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe1cac10-01f1-42fd-a4b7-cf08ce0d64be",
      "metadata": {
        "id": "fe1cac10-01f1-42fd-a4b7-cf08ce0d64be"
      },
      "outputs": [],
      "source": [
        "# check that the number of deviation scores matches the number of subjects in the test set\n",
        "# there should be one deviation score per subject (one line per subject), so we can\n",
        "# verify by counting the line numbers in the Z_predict.txt file\n",
        "! cat rh_MeanThickness_thickness/Z_predict.txt | wc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d2dbc0-e82f-4af5-91eb-dc8aa60f6ba7",
      "metadata": {
        "id": "88d2dbc0-e82f-4af5-91eb-dc8aa60f6ba7"
      },
      "source": [
        "The deviation scores are output as a text file in separate folders. We want to summarize the deviation scores across all models estimates so we can organize them into a single file, and merge the deviation scores into the original data file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3fb0ced-ed44-487c-86b8-07b9fc04d64e",
      "metadata": {
        "id": "e3fb0ced-ed44-487c-86b8-07b9fc04d64e"
      },
      "outputs": [],
      "source": [
        "! mkdir deviation_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "571f549e-9edd-4f8b-a6b3-d76cd23609f0",
      "metadata": {
        "id": "571f549e-9edd-4f8b-a6b3-d76cd23609f0"
      },
      "outputs": [],
      "source": [
        "! for i in *; do if [[ -e ${i}/Z_predict.txt ]]; then cp ${i}/Z_predict.txt deviation_scores/${i}_Z_predict.txt; fi; done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f63da6c-91e8-4852-91a7-a4e8bc9d9f31",
      "metadata": {
        "id": "9f63da6c-91e8-4852-91a7-a4e8bc9d9f31"
      },
      "outputs": [],
      "source": [
        "z_dir = '/content/braincharts/models/' + model_name + '/deviation_scores/'\n",
        "\n",
        "filelist = [name for name in os.listdir(z_dir)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8791195b-09a9-4251-8fd7-35e80a028d2f",
      "metadata": {
        "id": "8791195b-09a9-4251-8fd7-35e80a028d2f"
      },
      "outputs": [],
      "source": [
        "os.chdir(z_dir)\n",
        "Z_df = pd.concat([pd.read_csv(item, names=[item[:-4]]) for item in filelist], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1054959-dd17-4c1b-b1db-56cf849ecae2",
      "metadata": {
        "id": "f1054959-dd17-4c1b-b1db-56cf849ecae2"
      },
      "outputs": [],
      "source": [
        "df_te.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab00be4-d9c8-49aa-b407-f69946ca2d6c",
      "metadata": {
        "id": "6ab00be4-d9c8-49aa-b407-f69946ca2d6c"
      },
      "outputs": [],
      "source": [
        "Z_df['sub_id'] = df_te['sub_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6185b6-d9d7-4651-bbab-2cfba6c46963",
      "metadata": {
        "id": "9f6185b6-d9d7-4651-bbab-2cfba6c46963"
      },
      "outputs": [],
      "source": [
        "df_te_Z = pd.merge(df_te, Z_df, on='sub_id', how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae932714-60c3-4a36-8b72-cd9086a25761",
      "metadata": {
        "id": "ae932714-60c3-4a36-8b72-cd9086a25761"
      },
      "outputs": [],
      "source": [
        "df_te_Z.to_csv('PNCtoolkit_deviation_scores.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c97bd6e",
      "metadata": {
        "id": "9c97bd6e"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('PNCtoolkit_deviation_scores.csv')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "apply_normative_models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "f65f66140ab2d9a57fedc58a3b7e1d01f34d12111107cec87dc46b07c8179a15"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}